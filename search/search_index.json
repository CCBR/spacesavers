{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Spacesavers \u00b6 Disk space : the final frontier . This is the home of the cli, Spacesavers. Its long-term mission: to explore shared file systems, to seek old, duplicated files, and to boldly report disk space usage like no bot before! Overview \u00b6 Welcome to spacesaver's documentation! This guide is the main source of documentation for users that are getting started with spacesaver . The ./spacesaver command line tool is composed several inter-related sub commands to evaluate and optimize disk space usage across different shared group areas. Each of the available sub commands perform different functions analogous to common unix commands to optimize and report disk space usage: spacesaver ls : Recusively list directory contents to find duplicated files spacesaver df : Report disk space usage, duplication rate, and score path spacesaver ln : Replace duplicated files with hard links to save disk space Spacesavers can be utilized to recursively find duplicate files, report duplicated disk space usage, or replace duplicated files with hard links. Before getting started, we highly recommend reading through the usage section of each available sub command. For more information about issues or trouble-shooting a problem, please checkout our FAQ prior to opening an issue on Github . Contribute \u00b6 This site is a living document, created for and by members like you. Spacesavers is maintained by the members of CCBR and is improved by continous feedback! We encourage you to contribute new content and make improvements to existing content via pull request to our GitHub repository .","title":"About"},{"location":"#spacesavers","text":"Disk space : the final frontier . This is the home of the cli, Spacesavers. Its long-term mission: to explore shared file systems, to seek old, duplicated files, and to boldly report disk space usage like no bot before!","title":"Spacesavers"},{"location":"#overview","text":"Welcome to spacesaver's documentation! This guide is the main source of documentation for users that are getting started with spacesaver . The ./spacesaver command line tool is composed several inter-related sub commands to evaluate and optimize disk space usage across different shared group areas. Each of the available sub commands perform different functions analogous to common unix commands to optimize and report disk space usage: spacesaver ls : Recusively list directory contents to find duplicated files spacesaver df : Report disk space usage, duplication rate, and score path spacesaver ln : Replace duplicated files with hard links to save disk space Spacesavers can be utilized to recursively find duplicate files, report duplicated disk space usage, or replace duplicated files with hard links. Before getting started, we highly recommend reading through the usage section of each available sub command. For more information about issues or trouble-shooting a problem, please checkout our FAQ prior to opening an issue on Github .","title":"Overview"},{"location":"#contribute","text":"This site is a living document, created for and by members like you. Spacesavers is maintained by the members of CCBR and is improved by continous feedback! We encourage you to contribute new content and make improvements to existing content via pull request to our GitHub repository .","title":"Contribute"},{"location":"license/","text":"MIT License \u00b6 Copyright \u00a9 2021 CCBR Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#mit-license","text":"Copyright \u00a9 2021 CCBR Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"MIT License"},{"location":"faq/questions/","text":"Frequently Asked Questions \u00b6 Coming soon!","title":"General Questions"},{"location":"faq/questions/#frequently-asked-questions","text":"Coming soon!","title":"Frequently Asked Questions"},{"location":"misc/CRAMbasics/","text":"CRAM file format (lossless only) \u00b6 It is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Alignments should be kept in chromosome/position sort order., i.e., CRAM is always coordinate sorted. The reference must be available at all times. Losing it may be equivalent to losing all your read sequences. The reference sequence is linked to by the md5sum (M5 auxiliary tag) in the CRAM header ( @SQ tags). This is mandatory and part of the CRAM specification. In SAM/BAM format, these M5 tags are optional. Therefore converting from SAM/BAM to CRAM requires some additional overhead to link the CRAM to the correct reference sequence. SAM/BAM to CRAM conversion \u00b6 % samtools view -T ref.fasta -C -o test.cram test.bam Things to note about -T option: the ref.fasta file needs to be FASTA format and can optionally compressed by bgzip . it should be indexed by samtools faidx . If an index is not present one will be generated if the reference file is local. it can be \"not-local\" and be a https://, s3:// or other URL. If it is \"not-local\" then the faidx file should also ideally be at the same location If the BAM file already has M5 and UR tags then the -T can be dropped. % samtools view -C -o test.cram test.bam How to add M5 and UR tags to bam file?? : One solution is to use Bamutil on biowulf: % bam polishBAM --fasta ref.fa --in test.bam --out test.polished.bam Here, test.polished.bam is supposed to have M5 and UR tags ... but this failed in testing. In testing, test.bam and test.polished.bam seem to be identical.... Weird! Viewing CRAM files or converting them to SAM/BAM \u00b6 Again, the reference is required. The UR tag in CRAM file has the location of each reference sequence. Each sequences MD5Sum is also saved as a M5 tag in the @SQ header line. If the original sequence has moved or has been renamed then retrieval is almost impossible. Two possible solutions to this: save the reference sequences in a immutable s3 bucket (add and forget) create a local cache of reference sequences. This can be done as follows: use the seq_cache_populate.pl script bundled with samtools % seq_cache_populate.pl --root /path/to/common/folder ref.fasta Export the location of this common folder via REF_CACHE env variable % export REF_CACHE = /path/to/common/folder/%2s/%2s/%s Off course, if you know the location of the reference then, % samtools view -b -T /path/to/ref.fasta -o test.bam -@8 test.cram Some testing \u00b6 bam to cram % samtools view -T ../../resources/mm10/other/mm10.plus_rDNA_plus_5S.id90_masked.rRNA_pseudogenes_masked.fa -C -o WT3_resent.minus.toSNPcalling.cram WT3_resent.minus.toSNPcalling.bam size difference % ls -arlth WT3_resent.minus.toSNPcalling.*am -rw-rw-r-- 1 kopardevn RBL_NCI 88M Sep 7 15 :14 WT3_resent.minus.toSNPcalling.bam -rw-rw-r-- 1 kopardevn RBL_NCI 27M Sep 15 21 :35 WT3_resent.minus.toSNPcalling.cram bam to cram to bam % samtools view -C -o WT3_resent.minus.toSNPcalling.cram2bam2cram.cram WT3_resent.minus.toSNPcalling.cram2bam.bam size differences % ls -arlth WT3_resent.minus.toSNPcalling.*am -rw-rw-r-- 1 kopardevn RBL_NCI 88M Sep 7 15 :14 WT3_resent.minus.toSNPcalling.bam -rw-rw-r-- 1 kopardevn RBL_NCI 27M Sep 15 21 :35 WT3_resent.minus.toSNPcalling.cram -rw-rw-r-- 1 kopardevn RBL_NCI 92M Sep 15 21 :39 WT3_resent.minus.toSNPcalling.cram2bam.bam header differences % for f in WT3_resent.minus.toSNPcalling.bam WT3_resent.minus.toSNPcalling.cram WT3_resent.minus.toSNPcalling.cram2bam.bam ; do echo $f ; samtools view -H $f | head -n5 ; done WT3_resent.minus.toSNPcalling.bam @HD VN:1.5 SO:coordinate @SQ SN:chr1 LN:195471971 @SQ SN:chr2 LN:182113224 @SQ SN:chr3 LN:160039680 @SQ SN:chr4 LN:156508116 WT3_resent.minus.toSNPcalling.cram @HD VN:1.5 SO:coordinate @SQ SN:chr1 LN:195471971 M5:155e60353e04620c1ae2a4273b5c980e UR:/gpfs/gsfs11/users/RBL_NCI/Wolin/mESC_slam_analysis/find_mutation_090721/star/../../resources/mm10/other/mm10.plus_rDNA_plus_5S.id90_masked.rRNA_pseudogenes_masked.fa @SQ SN:chr2 LN:182113224 M5:01ad8fdc245bd146c69dbeb97d9adeb2 UR:/gpfs/gsfs11/users/RBL_NCI/Wolin/mESC_slam_analysis/find_mutation_090721/star/../../resources/mm10/other/mm10.plus_rDNA_plus_5S.id90_masked.rRNA_pseudogenes_masked.fa @SQ SN:chr3 LN:160039680 M5:91ada520bad86f25781017213b7e007f UR:/gpfs/gsfs11/users/RBL_NCI/Wolin/mESC_slam_analysis/find_mutation_090721/star/../../resources/mm10/other/mm10.plus_rDNA_plus_5S.id90_masked.rRNA_pseudogenes_masked.fa @SQ SN:chr4 LN:156508116 M5:5a280a14bfb9a64ba8f3e80e5e3b5b90 UR:/gpfs/gsfs11/users/RBL_NCI/Wolin/mESC_slam_analysis/find_mutation_090721/star/../../resources/mm10/other/mm10.plus_rDNA_plus_5S.id90_masked.rRNA_pseudogenes_masked.fa WT3_resent.minus.toSNPcalling.cram2bam.bam @HD VN:1.5 SO:coordinate @SQ SN:chr1 LN:195471971 M5:155e60353e04620c1ae2a4273b5c980e UR:/gpfs/gsfs11/users/RBL_NCI/Wolin/mESC_slam_analysis/find_mutation_090721/star/../../resources/mm10/other/mm10.plus_rDNA_plus_5S.id90_masked.rRNA_pseudogenes_masked.fa @SQ SN:chr2 LN:182113224 M5:01ad8fdc245bd146c69dbeb97d9adeb2 UR:/gpfs/gsfs11/users/RBL_NCI/Wolin/mESC_slam_analysis/find_mutation_090721/star/../../resources/mm10/other/mm10.plus_rDNA_plus_5S.id90_masked.rRNA_pseudogenes_masked.fa @SQ SN:chr3 LN:160039680 M5:91ada520bad86f25781017213b7e007f UR:/gpfs/gsfs11/users/RBL_NCI/Wolin/mESC_slam_analysis/find_mutation_090721/star/../../resources/mm10/other/mm10.plus_rDNA_plus_5S.id90_masked.rRNA_pseudogenes_masked.fa @SQ SN:chr4 LN:156508116 M5:5a280a14bfb9a64ba8f3e80e5e3b5b90 UR:/gpfs/gsfs11/users/RBL_NCI/Wolin/mESC_slam_analysis/find_mutation_090721/star/../../resources/mm10/other/mm10.plus_rDNA_plus_5S.id90_masked.rRNA_pseudogenes_masked.fa CRAM file has the M5 and UR tags in @SQ header lines. BAM created from CRAM also has these tags. (such BAMs can be directly converted to CRAM without the -T argument) Testing seq_cache_populate.pl \u00b6 Using test.fa for testing: % cat tmp.fa >5S GTCTACGGCCATACCACCCTGAACGCGCCCGATCTCGTCTGATCTCGGAAGCTAAGCAGGGTCGGGCCTGGTTAGTACTTGGATGGGAGACCGCCTGGGAATACCGGGTGCTGTAGGCTTTGGACTCCCCTCTGTCTCTCTCTCCCTTTT Add it to REF_CACHE % seq_cache_populate.pl --root /lscratch/ $SLURM_JOBID /ref_path ./test.fa Reading ./test.fa ... /lscratch/23025678/ref_path/2a/c6/36dca37eb7a6f2f222c7b5ef5e96 5S Use environment REF_CACHE = /lscratch/23025678/ref_path/%2s/%2s/%s for accessing these files. See also https://www.htslib.org/workflow/#the-ref_path-and-ref_cache for further information. Per line sequence width does not matter % module load fastxtoolkit % fasta_formatter -i test.fa -w 80 -o test.w80.fa % more test.w80.fa >5S GTCTACGGCCATACCACCCTGAACGCGCCCGATCTCGTCTGATCTCGGAAGCTAAGCAGGGTCGGGCCTGGTTAGTACTT GGATGGGAGACCGCCTGGGAATACCGGGTGCTGTAGGCTTTGGACTCCCCTCTGTCTCTCTCTCCCTTTT % seq_cache_populate.pl --root /lscratch/ $SLURM_JOBID /ref_path ./test.w80.fa Reading ./test.w80.fa ... Already exists: 2ac636dca37eb7a6f2f222c7b5ef5e96 5S Use environment REF_CACHE = /lscratch/23025678/ref_path/%2s/%2s/%s for accessing these files. See also https://www.htslib.org/workflow/#the-ref_path-and-ref_cache for further information. Sequence description does not matter # adding description to sequence name % cat test.desc.fa >5S Some Description GTCTACGGCCATACCACCCTGAACGCGCCCGATCTCGTCTGATCTCGGAAGCTAAGCAGGGTCGGGCCTGGTTAGTACTTGGATGGGAGACCGCCTGGGAATACCGGGTGCTGTAGGCTTTGGACTCCCCTCTGTCTCTCTCTCCCTTTT % seq_cache_populate.pl --root /lscratch/ $SLURM_JOBID /ref_path test.desc.fa Reading test.desc.fa ... Already exists: 2ac636dca37eb7a6f2f222c7b5ef5e96 5S Use environment REF_CACHE = /lscratch/23025678/ref_path/%2s/%2s/%s for accessing these files. See also https://www.htslib.org/workflow/#the-ref_path-and-ref_cache for further information. Changing sequence name (ID) does not matter % cat test.newid.fa >chr5S GTCTACGGCCATACCACCCTGAACGCGCCCGATCTCGTCTGATCTCGGAAGCTAAGCAGGGTCGGGCCTGGTTAGTACTTGGATGGGAGACCGCCTGGGAATACCGGGTGCTGTAGGCTTTGGACTCCCCTCTGTCTCTCTCTCCCTTTT % seq_cache_populate.pl --root /lscratch/ $SLURM_JOBID /ref_path test.newid.fa Reading test.newid.fa ... Already exists: 2ac636dca37eb7a6f2f222c7b5ef5e96 chr5S Use environment REF_CACHE = /lscratch/23025678/ref_path/%2s/%2s/%s for accessing these files. See also https://www.htslib.org/workflow/#the-ref_path-and-ref_cache for further information.","title":"CRAMbasics"},{"location":"misc/CRAMbasics/#cram-file-format-lossless-only","text":"It is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Alignments should be kept in chromosome/position sort order., i.e., CRAM is always coordinate sorted. The reference must be available at all times. Losing it may be equivalent to losing all your read sequences. The reference sequence is linked to by the md5sum (M5 auxiliary tag) in the CRAM header ( @SQ tags). This is mandatory and part of the CRAM specification. In SAM/BAM format, these M5 tags are optional. Therefore converting from SAM/BAM to CRAM requires some additional overhead to link the CRAM to the correct reference sequence.","title":"CRAM file format (lossless only)"},{"location":"misc/CRAMbasics/#sambam-to-cram-conversion","text":"% samtools view -T ref.fasta -C -o test.cram test.bam Things to note about -T option: the ref.fasta file needs to be FASTA format and can optionally compressed by bgzip . it should be indexed by samtools faidx . If an index is not present one will be generated if the reference file is local. it can be \"not-local\" and be a https://, s3:// or other URL. If it is \"not-local\" then the faidx file should also ideally be at the same location If the BAM file already has M5 and UR tags then the -T can be dropped. % samtools view -C -o test.cram test.bam How to add M5 and UR tags to bam file?? : One solution is to use Bamutil on biowulf: % bam polishBAM --fasta ref.fa --in test.bam --out test.polished.bam Here, test.polished.bam is supposed to have M5 and UR tags ... but this failed in testing. In testing, test.bam and test.polished.bam seem to be identical.... Weird!","title":"SAM/BAM to CRAM conversion"},{"location":"misc/CRAMbasics/#viewing-cram-files-or-converting-them-to-sambam","text":"Again, the reference is required. The UR tag in CRAM file has the location of each reference sequence. Each sequences MD5Sum is also saved as a M5 tag in the @SQ header line. If the original sequence has moved or has been renamed then retrieval is almost impossible. Two possible solutions to this: save the reference sequences in a immutable s3 bucket (add and forget) create a local cache of reference sequences. This can be done as follows: use the seq_cache_populate.pl script bundled with samtools % seq_cache_populate.pl --root /path/to/common/folder ref.fasta Export the location of this common folder via REF_CACHE env variable % export REF_CACHE = /path/to/common/folder/%2s/%2s/%s Off course, if you know the location of the reference then, % samtools view -b -T /path/to/ref.fasta -o test.bam -@8 test.cram","title":"Viewing CRAM files or converting them to SAM/BAM"},{"location":"misc/CRAMbasics/#some-testing","text":"bam to cram % samtools view -T ../../resources/mm10/other/mm10.plus_rDNA_plus_5S.id90_masked.rRNA_pseudogenes_masked.fa -C -o WT3_resent.minus.toSNPcalling.cram WT3_resent.minus.toSNPcalling.bam size difference % ls -arlth WT3_resent.minus.toSNPcalling.*am -rw-rw-r-- 1 kopardevn RBL_NCI 88M Sep 7 15 :14 WT3_resent.minus.toSNPcalling.bam -rw-rw-r-- 1 kopardevn RBL_NCI 27M Sep 15 21 :35 WT3_resent.minus.toSNPcalling.cram bam to cram to bam % samtools view -C -o WT3_resent.minus.toSNPcalling.cram2bam2cram.cram WT3_resent.minus.toSNPcalling.cram2bam.bam size differences % ls -arlth WT3_resent.minus.toSNPcalling.*am -rw-rw-r-- 1 kopardevn RBL_NCI 88M Sep 7 15 :14 WT3_resent.minus.toSNPcalling.bam -rw-rw-r-- 1 kopardevn RBL_NCI 27M Sep 15 21 :35 WT3_resent.minus.toSNPcalling.cram -rw-rw-r-- 1 kopardevn RBL_NCI 92M Sep 15 21 :39 WT3_resent.minus.toSNPcalling.cram2bam.bam header differences % for f in WT3_resent.minus.toSNPcalling.bam WT3_resent.minus.toSNPcalling.cram WT3_resent.minus.toSNPcalling.cram2bam.bam ; do echo $f ; samtools view -H $f | head -n5 ; done WT3_resent.minus.toSNPcalling.bam @HD VN:1.5 SO:coordinate @SQ SN:chr1 LN:195471971 @SQ SN:chr2 LN:182113224 @SQ SN:chr3 LN:160039680 @SQ SN:chr4 LN:156508116 WT3_resent.minus.toSNPcalling.cram @HD VN:1.5 SO:coordinate @SQ SN:chr1 LN:195471971 M5:155e60353e04620c1ae2a4273b5c980e UR:/gpfs/gsfs11/users/RBL_NCI/Wolin/mESC_slam_analysis/find_mutation_090721/star/../../resources/mm10/other/mm10.plus_rDNA_plus_5S.id90_masked.rRNA_pseudogenes_masked.fa @SQ SN:chr2 LN:182113224 M5:01ad8fdc245bd146c69dbeb97d9adeb2 UR:/gpfs/gsfs11/users/RBL_NCI/Wolin/mESC_slam_analysis/find_mutation_090721/star/../../resources/mm10/other/mm10.plus_rDNA_plus_5S.id90_masked.rRNA_pseudogenes_masked.fa @SQ SN:chr3 LN:160039680 M5:91ada520bad86f25781017213b7e007f UR:/gpfs/gsfs11/users/RBL_NCI/Wolin/mESC_slam_analysis/find_mutation_090721/star/../../resources/mm10/other/mm10.plus_rDNA_plus_5S.id90_masked.rRNA_pseudogenes_masked.fa @SQ SN:chr4 LN:156508116 M5:5a280a14bfb9a64ba8f3e80e5e3b5b90 UR:/gpfs/gsfs11/users/RBL_NCI/Wolin/mESC_slam_analysis/find_mutation_090721/star/../../resources/mm10/other/mm10.plus_rDNA_plus_5S.id90_masked.rRNA_pseudogenes_masked.fa WT3_resent.minus.toSNPcalling.cram2bam.bam @HD VN:1.5 SO:coordinate @SQ SN:chr1 LN:195471971 M5:155e60353e04620c1ae2a4273b5c980e UR:/gpfs/gsfs11/users/RBL_NCI/Wolin/mESC_slam_analysis/find_mutation_090721/star/../../resources/mm10/other/mm10.plus_rDNA_plus_5S.id90_masked.rRNA_pseudogenes_masked.fa @SQ SN:chr2 LN:182113224 M5:01ad8fdc245bd146c69dbeb97d9adeb2 UR:/gpfs/gsfs11/users/RBL_NCI/Wolin/mESC_slam_analysis/find_mutation_090721/star/../../resources/mm10/other/mm10.plus_rDNA_plus_5S.id90_masked.rRNA_pseudogenes_masked.fa @SQ SN:chr3 LN:160039680 M5:91ada520bad86f25781017213b7e007f UR:/gpfs/gsfs11/users/RBL_NCI/Wolin/mESC_slam_analysis/find_mutation_090721/star/../../resources/mm10/other/mm10.plus_rDNA_plus_5S.id90_masked.rRNA_pseudogenes_masked.fa @SQ SN:chr4 LN:156508116 M5:5a280a14bfb9a64ba8f3e80e5e3b5b90 UR:/gpfs/gsfs11/users/RBL_NCI/Wolin/mESC_slam_analysis/find_mutation_090721/star/../../resources/mm10/other/mm10.plus_rDNA_plus_5S.id90_masked.rRNA_pseudogenes_masked.fa CRAM file has the M5 and UR tags in @SQ header lines. BAM created from CRAM also has these tags. (such BAMs can be directly converted to CRAM without the -T argument)","title":"Some testing"},{"location":"misc/CRAMbasics/#testing-seq_cache_populatepl","text":"Using test.fa for testing: % cat tmp.fa >5S GTCTACGGCCATACCACCCTGAACGCGCCCGATCTCGTCTGATCTCGGAAGCTAAGCAGGGTCGGGCCTGGTTAGTACTTGGATGGGAGACCGCCTGGGAATACCGGGTGCTGTAGGCTTTGGACTCCCCTCTGTCTCTCTCTCCCTTTT Add it to REF_CACHE % seq_cache_populate.pl --root /lscratch/ $SLURM_JOBID /ref_path ./test.fa Reading ./test.fa ... /lscratch/23025678/ref_path/2a/c6/36dca37eb7a6f2f222c7b5ef5e96 5S Use environment REF_CACHE = /lscratch/23025678/ref_path/%2s/%2s/%s for accessing these files. See also https://www.htslib.org/workflow/#the-ref_path-and-ref_cache for further information. Per line sequence width does not matter % module load fastxtoolkit % fasta_formatter -i test.fa -w 80 -o test.w80.fa % more test.w80.fa >5S GTCTACGGCCATACCACCCTGAACGCGCCCGATCTCGTCTGATCTCGGAAGCTAAGCAGGGTCGGGCCTGGTTAGTACTT GGATGGGAGACCGCCTGGGAATACCGGGTGCTGTAGGCTTTGGACTCCCCTCTGTCTCTCTCTCCCTTTT % seq_cache_populate.pl --root /lscratch/ $SLURM_JOBID /ref_path ./test.w80.fa Reading ./test.w80.fa ... Already exists: 2ac636dca37eb7a6f2f222c7b5ef5e96 5S Use environment REF_CACHE = /lscratch/23025678/ref_path/%2s/%2s/%s for accessing these files. See also https://www.htslib.org/workflow/#the-ref_path-and-ref_cache for further information. Sequence description does not matter # adding description to sequence name % cat test.desc.fa >5S Some Description GTCTACGGCCATACCACCCTGAACGCGCCCGATCTCGTCTGATCTCGGAAGCTAAGCAGGGTCGGGCCTGGTTAGTACTTGGATGGGAGACCGCCTGGGAATACCGGGTGCTGTAGGCTTTGGACTCCCCTCTGTCTCTCTCTCCCTTTT % seq_cache_populate.pl --root /lscratch/ $SLURM_JOBID /ref_path test.desc.fa Reading test.desc.fa ... Already exists: 2ac636dca37eb7a6f2f222c7b5ef5e96 5S Use environment REF_CACHE = /lscratch/23025678/ref_path/%2s/%2s/%s for accessing these files. See also https://www.htslib.org/workflow/#the-ref_path-and-ref_cache for further information. Changing sequence name (ID) does not matter % cat test.newid.fa >chr5S GTCTACGGCCATACCACCCTGAACGCGCCCGATCTCGTCTGATCTCGGAAGCTAAGCAGGGTCGGGCCTGGTTAGTACTTGGATGGGAGACCGCCTGGGAATACCGGGTGCTGTAGGCTTTGGACTCCCCTCTGTCTCTCTCTCCCTTTT % seq_cache_populate.pl --root /lscratch/ $SLURM_JOBID /ref_path test.newid.fa Reading test.newid.fa ... Already exists: 2ac636dca37eb7a6f2f222c7b5ef5e96 chr5S Use environment REF_CACHE = /lscratch/23025678/ref_path/%2s/%2s/%s for accessing these files. See also https://www.htslib.org/workflow/#the-ref_path-and-ref_cache for further information.","title":"Testing seq_cache_populate.pl"},{"location":"usage/df/","text":"./spacesaver df \u00b6 About \u00b6 The ./spacesaver executable is composed of several inter-related sub commands. Please see ./spacesaver -h for all available options. This part of the documentation describes options, concepts, and output for ./spacesaver df sub command in more detail. ./spacesaver df can be used to report duplicated disk usage. The output of this command is similar to the unix df -h command. Internally this command calls the ./spacesaver ls command to determine the extent of duplication in a given directory. This command also accepts standard input where the output of ls sub command can be piped into the df command. A duplication rate is calculated for each of the provided paths to assess the amount of redudant data in a given location. A weighted score, ranging from 0-100, is also assigned to each provided path where the higher the score, the better! ./spacesaver df only has one required input , a path or set of paths. Synopsis \u00b6 $ spacesaver df [-h] DIRECTORY [DIRECTORY ...] The synopsis for each command shows its parameters and their usage. Optional parameters are shown in square brackets. The df sub command takes one or more directories as input. For a given directory, its disk space usage will be reported and assessed. This command can be used to summarize the output from the from the ls sub command into a few easy to interpret metrics. The metrics generated from this command can be used to identify directories which may be candidates for deduplication or for archiving in deeper storage. Please note that symlinks or multiple occurences of hard links will be skipped over as these files do not take up any appreciable disk space. Only one instance of a set of hard links pointing to the same inode will be reported. Use you can always use the -h option for information on a specific command. Required Arguments \u00b6 Each of the following arguments are required. Failure to provide a required argument will result in a non-zero exit-code. DIRECTORY [DIRECTORY ...] Input directories to find duplicates. type: path One or more directories can be provided as positional arguments. From the command-line, each directory should seperated by a space. Globbing is supported! This makes selecting paths easier. Example: /data/CCBR/rawdata/ccbr123/ Optional Arguments \u00b6 Each of the following arguments are optional and do not need to be provided. -h, --help Display Help. type: boolean Shows command's synopsis, help message, and an example command Example: --help Output \u00b6 The output of the df sub command is similar to the unix display free disk space command with more information. Just like the ls sub command, it is displayed to standard ouput. Here is a description of each column's output: Column Name Example Value 1 Path /data/CCBR/rawdata/ccbr123/ 2 FolderOwner Guido van Rossum 3 FileCoOwners finneyr[96.297%]|maggiec[3.703%] 4 Duplicated 177.316 GiB 5 Duplicated_Bytes 190391226983 6 Used 1.456 TiB 7 %Duplicated 0.0% 8 wAgeS 12.4 9 wDupS 0.0 10 wOccS 5.1 11 Score 82.5 Please note: The output is seperated or delimited by tabs: \\t . %Duplciated is reported as the DuplicatedBytes/TotalBytes . A Score is also assigned to a given path where the higher the score, the better. A Score of 0 would indicate that all the files are older than 2.7 years AND all the files are duplicates AND the files make up more than 10% of our shared group area. wAgeS , wDupS , and wOccS are the weighted indiviudal components that make up the Score listed in column 11. Example \u00b6 Please note this sub command may take a while to process depending on the shear number or the size of files existing in a given sub tree. As so, this command should not be run on the head node! Please allocate an interactive node prior to running this command or submit this command as a job via sbatch. # Step 0.) Grab an interactive node # Do not run this on the head node! srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 16gb --cpus-per-task = 4 --pty bash module purge # Option 1.) Repprt disk space usage of a directory ./spacesaver df /data/CCBR/rawdata/ccbr123/ # Option 2.) Read ls sub command from standard input ./spacesaver ls /data/CCBR/rawdata/ccbr123/ > ccbr123_ls.tsv ./cat ccbr123_ls.tsv | spacesaver df /data/CCBR/rawdata/ccbr123/","title":"spacesaver df"},{"location":"usage/df/#spacesaver-df","text":"","title":"./spacesaver df"},{"location":"usage/df/#about","text":"The ./spacesaver executable is composed of several inter-related sub commands. Please see ./spacesaver -h for all available options. This part of the documentation describes options, concepts, and output for ./spacesaver df sub command in more detail. ./spacesaver df can be used to report duplicated disk usage. The output of this command is similar to the unix df -h command. Internally this command calls the ./spacesaver ls command to determine the extent of duplication in a given directory. This command also accepts standard input where the output of ls sub command can be piped into the df command. A duplication rate is calculated for each of the provided paths to assess the amount of redudant data in a given location. A weighted score, ranging from 0-100, is also assigned to each provided path where the higher the score, the better! ./spacesaver df only has one required input , a path or set of paths.","title":"About"},{"location":"usage/df/#synopsis","text":"$ spacesaver df [-h] DIRECTORY [DIRECTORY ...] The synopsis for each command shows its parameters and their usage. Optional parameters are shown in square brackets. The df sub command takes one or more directories as input. For a given directory, its disk space usage will be reported and assessed. This command can be used to summarize the output from the from the ls sub command into a few easy to interpret metrics. The metrics generated from this command can be used to identify directories which may be candidates for deduplication or for archiving in deeper storage. Please note that symlinks or multiple occurences of hard links will be skipped over as these files do not take up any appreciable disk space. Only one instance of a set of hard links pointing to the same inode will be reported. Use you can always use the -h option for information on a specific command.","title":"Synopsis"},{"location":"usage/df/#required-arguments","text":"Each of the following arguments are required. Failure to provide a required argument will result in a non-zero exit-code. DIRECTORY [DIRECTORY ...] Input directories to find duplicates. type: path One or more directories can be provided as positional arguments. From the command-line, each directory should seperated by a space. Globbing is supported! This makes selecting paths easier. Example: /data/CCBR/rawdata/ccbr123/","title":"Required Arguments"},{"location":"usage/df/#optional-arguments","text":"Each of the following arguments are optional and do not need to be provided. -h, --help Display Help. type: boolean Shows command's synopsis, help message, and an example command Example: --help","title":"Optional Arguments"},{"location":"usage/df/#output","text":"The output of the df sub command is similar to the unix display free disk space command with more information. Just like the ls sub command, it is displayed to standard ouput. Here is a description of each column's output: Column Name Example Value 1 Path /data/CCBR/rawdata/ccbr123/ 2 FolderOwner Guido van Rossum 3 FileCoOwners finneyr[96.297%]|maggiec[3.703%] 4 Duplicated 177.316 GiB 5 Duplicated_Bytes 190391226983 6 Used 1.456 TiB 7 %Duplicated 0.0% 8 wAgeS 12.4 9 wDupS 0.0 10 wOccS 5.1 11 Score 82.5 Please note: The output is seperated or delimited by tabs: \\t . %Duplciated is reported as the DuplicatedBytes/TotalBytes . A Score is also assigned to a given path where the higher the score, the better. A Score of 0 would indicate that all the files are older than 2.7 years AND all the files are duplicates AND the files make up more than 10% of our shared group area. wAgeS , wDupS , and wOccS are the weighted indiviudal components that make up the Score listed in column 11.","title":"Output"},{"location":"usage/df/#example","text":"Please note this sub command may take a while to process depending on the shear number or the size of files existing in a given sub tree. As so, this command should not be run on the head node! Please allocate an interactive node prior to running this command or submit this command as a job via sbatch. # Step 0.) Grab an interactive node # Do not run this on the head node! srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 16gb --cpus-per-task = 4 --pty bash module purge # Option 1.) Repprt disk space usage of a directory ./spacesaver df /data/CCBR/rawdata/ccbr123/ # Option 2.) Read ls sub command from standard input ./spacesaver ls /data/CCBR/rawdata/ccbr123/ > ccbr123_ls.tsv ./cat ccbr123_ls.tsv | spacesaver df /data/CCBR/rawdata/ccbr123/","title":"Example"},{"location":"usage/ln/","text":"./spacesaver ln \u00b6 About \u00b6 Warning This sub command should only be run by advanced users. The ./spacesaver executable is composed of several inter-related sub commands. Please see ./spacesaver -h for all available options. This part of the documentation describes options, concepts, and output for ./spacesaver ln sub command in more detail. ./spacesaver ln can be used to remove duplicate files. To save diskspace, hardlinks will be created between any encountered duplicate files. The oldest occurence from the set of duplicated files will be used as a template. From this file, new hardlinks will be created. Hard links point to the same node on a device or file system, and do not take up any appreciable disk space. A significant amount of disk space can be recovered or saved by replacing duplicated files with hard links. Hard links come with a few caveats! Please read through the section below to understand any issues that can a rise when creating multiple hard links: Hard links cannot be created across different devices, volumes, or file systems. Some file systems do not support multiple hard links, such as FAT; however, most modern POSIX complaint operating systems such as linux or macOS support this feature. Creating multiple hard links has the effect of giving one file multiple names. From the file system's perspective, they are all the same. Each hard link will independently point to the same data on disk. If you update one hard link, the changes will propagate to all the other hard links. This causes an alias effect which can lead to desired or even catastrophic results. Disclaimer Use a healthy amount of caution and common sense when running this command. Seriously, with great power comes great responsibility! Treat it with the same respect as an rm command. If you do not fully understand the conditions described above, you should not run spacesaver ln . The -m option can be used to set a minimum file size in bytes. If a file does not exceed this minimum file size, then a hard link will not be created! Synopsis \u00b6 $ spacesaver ln [-h] [-m MINSIZE] DIRECTORY [DIRECTORY ...] The synopsis for each command shows its parameters and their usage. Optional parameters are shown in square brackets. The ln sub command takes one or more directories as input. Within a given directory, duplicated files will be replaced with hardlinks. It is advised to always run the spacesaver ls prior to running this command. Please note that symlinks or multiple occurences of hard links will be skipped over as these files do not take up any appreciable disk space. Only one instance of a set of hard links pointing to the same inode will be evaluated. Use you can always use the -h option for information on a specific command. Required Arguments \u00b6 Each of the following arguments are required. Failure to provide a required argument will result in a non-zero exit-code. DIRECTORY [DIRECTORY ...] Input directories to find duplicates. type: path One or more directories can be provided as positional arguments. From the command-line, each directory should seperated by a space. Globbing is supported! This makes selecting paths easier. Please note that duplicates are reported and replaced relative to other files within a provided directory. Example: /data/CCBR/rawdata/ccbr123/ Optional Arguments \u00b6 Each of the following arguments are optional and do not need to be provided. -h, --help Display Help. type: boolean Shows command's synopsis, help message, and an example command Example: --help -m Minimum size of a file in bytes. type: int default: 10485760 To create a hard link, the size of a given duplicated file must exceed this value. The default file size is 10 MiB. A file smaller than this default will not get replaced with a hardlink. Example: -m 1073741824 Example \u00b6 Please note that this sub command may take a while to process depending on the shear number or the size of files existing in a given subtree. As so, this command should not be run on the head node! Please allocate an interactive node prior to running this command or submit this command as a job via sbatch. To assess the number or size of files that exist in a provided directory, please run the spacesaver ls prior to running spacesaver ln . # Step 0.) Grab an interactive node # Do not run this on the head node! srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 16gb --cpus-per-task = 4 --pty bash module purge # Step 1.) Find duplicate files ./spacesaver ls /data/CCBR/rawdata/ccbr123/ > ccbr123_ls.tsv # Step 1A.) Option: Take a peek # at any identified duplicates more ccbr123_ls.tsv # Step 2.) Replace duplicate files # that are greater than 1 GiB in # size with hard links ./spacesaver ln -m 1073741824 /data/CCBR/ccbr123/rawdata/","title":"spacesaver ln"},{"location":"usage/ln/#spacesaver-ln","text":"","title":"./spacesaver ln"},{"location":"usage/ln/#about","text":"Warning This sub command should only be run by advanced users. The ./spacesaver executable is composed of several inter-related sub commands. Please see ./spacesaver -h for all available options. This part of the documentation describes options, concepts, and output for ./spacesaver ln sub command in more detail. ./spacesaver ln can be used to remove duplicate files. To save diskspace, hardlinks will be created between any encountered duplicate files. The oldest occurence from the set of duplicated files will be used as a template. From this file, new hardlinks will be created. Hard links point to the same node on a device or file system, and do not take up any appreciable disk space. A significant amount of disk space can be recovered or saved by replacing duplicated files with hard links. Hard links come with a few caveats! Please read through the section below to understand any issues that can a rise when creating multiple hard links: Hard links cannot be created across different devices, volumes, or file systems. Some file systems do not support multiple hard links, such as FAT; however, most modern POSIX complaint operating systems such as linux or macOS support this feature. Creating multiple hard links has the effect of giving one file multiple names. From the file system's perspective, they are all the same. Each hard link will independently point to the same data on disk. If you update one hard link, the changes will propagate to all the other hard links. This causes an alias effect which can lead to desired or even catastrophic results. Disclaimer Use a healthy amount of caution and common sense when running this command. Seriously, with great power comes great responsibility! Treat it with the same respect as an rm command. If you do not fully understand the conditions described above, you should not run spacesaver ln . The -m option can be used to set a minimum file size in bytes. If a file does not exceed this minimum file size, then a hard link will not be created!","title":"About"},{"location":"usage/ln/#synopsis","text":"$ spacesaver ln [-h] [-m MINSIZE] DIRECTORY [DIRECTORY ...] The synopsis for each command shows its parameters and their usage. Optional parameters are shown in square brackets. The ln sub command takes one or more directories as input. Within a given directory, duplicated files will be replaced with hardlinks. It is advised to always run the spacesaver ls prior to running this command. Please note that symlinks or multiple occurences of hard links will be skipped over as these files do not take up any appreciable disk space. Only one instance of a set of hard links pointing to the same inode will be evaluated. Use you can always use the -h option for information on a specific command.","title":"Synopsis"},{"location":"usage/ln/#required-arguments","text":"Each of the following arguments are required. Failure to provide a required argument will result in a non-zero exit-code. DIRECTORY [DIRECTORY ...] Input directories to find duplicates. type: path One or more directories can be provided as positional arguments. From the command-line, each directory should seperated by a space. Globbing is supported! This makes selecting paths easier. Please note that duplicates are reported and replaced relative to other files within a provided directory. Example: /data/CCBR/rawdata/ccbr123/","title":"Required Arguments"},{"location":"usage/ln/#optional-arguments","text":"Each of the following arguments are optional and do not need to be provided. -h, --help Display Help. type: boolean Shows command's synopsis, help message, and an example command Example: --help -m Minimum size of a file in bytes. type: int default: 10485760 To create a hard link, the size of a given duplicated file must exceed this value. The default file size is 10 MiB. A file smaller than this default will not get replaced with a hardlink. Example: -m 1073741824","title":"Optional Arguments"},{"location":"usage/ln/#example","text":"Please note that this sub command may take a while to process depending on the shear number or the size of files existing in a given subtree. As so, this command should not be run on the head node! Please allocate an interactive node prior to running this command or submit this command as a job via sbatch. To assess the number or size of files that exist in a provided directory, please run the spacesaver ls prior to running spacesaver ln . # Step 0.) Grab an interactive node # Do not run this on the head node! srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 16gb --cpus-per-task = 4 --pty bash module purge # Step 1.) Find duplicate files ./spacesaver ls /data/CCBR/rawdata/ccbr123/ > ccbr123_ls.tsv # Step 1A.) Option: Take a peek # at any identified duplicates more ccbr123_ls.tsv # Step 2.) Replace duplicate files # that are greater than 1 GiB in # size with hard links ./spacesaver ln -m 1073741824 /data/CCBR/ccbr123/rawdata/","title":"Example"},{"location":"usage/ls/","text":"./spacesaver ls \u00b6 About \u00b6 The ./spacesaver executable is composed of several inter-related sub commands. Please see ./spacesaver -h for all available options. This part of the documentation describes options, concepts, and output for ./spacesaver ls sub command in more detail. ./spacesaver ls can be used to identify duplicate files. The output of this command is similar to the unix ls -Rilath command; however, entries corresponding to duplicated files are collapsed into one line. To reduce overall strain on the file system and run time, a set of heuristics are used to filter a list of candidate duplicates prior to running computionally intensive steps. And as so, before calculating an MD5 checksum of the entire file, potential duplicates are identifed by matching their file sizes and the checksum of the file's first 64 KiB chunk. This significantly reduces the search search of the ls sub command prior to calculating an MD5 checksum of the entire file. ./spacesaver ls only has one required input , a path or set of paths. Synopsis \u00b6 $ spacesaver ls [-h] DIRECTORY [DIRECTORY ...] The synopsis for each command shows its parameters and their usage. Optional parameters are shown in square brackets. The ls sub command takes one or more directories as input. Within a given directory, each file will be recursively listed with its duplicates. Please note that symlinks or multiple occurences of hard links will be skipped over as these files do not take up any appreciable disk space. Only one instance of a set of hard links pointing to the same inode will be reported. Use you can always use the -h option for information on a specific command. Required Arguments \u00b6 Each of the following arguments are required. Failure to provide a required argument will result in a non-zero exit-code. DIRECTORY [DIRECTORY ...] Input directories to find duplicates. type: path One or more directories can be provided as positional arguments. From the command-line, each directory should seperated by a space. Globbing is supported! This makes selecting paths easier. Please note that duplicates are reported relative to other files within a directory. Example: /data/CCBR/rawdata/ccbr123/ Optional Arguments \u00b6 Each of the following arguments are optional and do not need to be provided. -h, --help Display Help. type: boolean Shows command's synopsis, help message, and an example command Example: --help Output \u00b6 The output of the ls sub command is similar to the unix long listing of a file with more information. It is displayed to standard ouput. Here is a description of each column's output: Column Name Example Value 1 Inode 1055643 2 Permissions -rw-rw---- 3 Owner kuhnsa 4 Group CCBR 5 Bytes 588895 6 Size 575.093 KiB 7 MDate 2021-09-20-17:20 8 File /path/to/oldest_duplicate.txt 9 NDuplicates 2 10 BDuplicates 1177790 11 SDuplicates 1.123 MiB 12 DOwners kuhnsa|kopardevn 13 Duplicates /path/to/dup1.txt|/path/to/dup2.txt Please note: The output is seperated or delimited by tabs: \\t , and columns containing multiple values for a list of files are seperated by a pipe: | . When reporting duplicates, one file is selected as the master copy. This is the oldest file from a set of duplicated files. The master copy is listed in Column 8, File . Any encountered duplicates will be reported in the last column, Duplicates . Example \u00b6 Please note this sub command may take a while to process depending on the shear number or the size of files existing in a given sub tree. As so, this command should not be run on the head node! Please allocate an interactive node prior to running this command or submit this command as a job via sbatch. # Step 0.) Grab an interactive node # Do not run this on the head node! srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 16gb --cpus-per-task = 4 --pty bash module purge # Step 1.) Find duplicate files ./spacesaver ls /data/CCBR/rawdata/ccbr123/ > ccbr123_ls.tsv","title":"spacesaver ls"},{"location":"usage/ls/#spacesaver-ls","text":"","title":"./spacesaver ls"},{"location":"usage/ls/#about","text":"The ./spacesaver executable is composed of several inter-related sub commands. Please see ./spacesaver -h for all available options. This part of the documentation describes options, concepts, and output for ./spacesaver ls sub command in more detail. ./spacesaver ls can be used to identify duplicate files. The output of this command is similar to the unix ls -Rilath command; however, entries corresponding to duplicated files are collapsed into one line. To reduce overall strain on the file system and run time, a set of heuristics are used to filter a list of candidate duplicates prior to running computionally intensive steps. And as so, before calculating an MD5 checksum of the entire file, potential duplicates are identifed by matching their file sizes and the checksum of the file's first 64 KiB chunk. This significantly reduces the search search of the ls sub command prior to calculating an MD5 checksum of the entire file. ./spacesaver ls only has one required input , a path or set of paths.","title":"About"},{"location":"usage/ls/#synopsis","text":"$ spacesaver ls [-h] DIRECTORY [DIRECTORY ...] The synopsis for each command shows its parameters and their usage. Optional parameters are shown in square brackets. The ls sub command takes one or more directories as input. Within a given directory, each file will be recursively listed with its duplicates. Please note that symlinks or multiple occurences of hard links will be skipped over as these files do not take up any appreciable disk space. Only one instance of a set of hard links pointing to the same inode will be reported. Use you can always use the -h option for information on a specific command.","title":"Synopsis"},{"location":"usage/ls/#required-arguments","text":"Each of the following arguments are required. Failure to provide a required argument will result in a non-zero exit-code. DIRECTORY [DIRECTORY ...] Input directories to find duplicates. type: path One or more directories can be provided as positional arguments. From the command-line, each directory should seperated by a space. Globbing is supported! This makes selecting paths easier. Please note that duplicates are reported relative to other files within a directory. Example: /data/CCBR/rawdata/ccbr123/","title":"Required Arguments"},{"location":"usage/ls/#optional-arguments","text":"Each of the following arguments are optional and do not need to be provided. -h, --help Display Help. type: boolean Shows command's synopsis, help message, and an example command Example: --help","title":"Optional Arguments"},{"location":"usage/ls/#output","text":"The output of the ls sub command is similar to the unix long listing of a file with more information. It is displayed to standard ouput. Here is a description of each column's output: Column Name Example Value 1 Inode 1055643 2 Permissions -rw-rw---- 3 Owner kuhnsa 4 Group CCBR 5 Bytes 588895 6 Size 575.093 KiB 7 MDate 2021-09-20-17:20 8 File /path/to/oldest_duplicate.txt 9 NDuplicates 2 10 BDuplicates 1177790 11 SDuplicates 1.123 MiB 12 DOwners kuhnsa|kopardevn 13 Duplicates /path/to/dup1.txt|/path/to/dup2.txt Please note: The output is seperated or delimited by tabs: \\t , and columns containing multiple values for a list of files are seperated by a pipe: | . When reporting duplicates, one file is selected as the master copy. This is the oldest file from a set of duplicated files. The master copy is listed in Column 8, File . Any encountered duplicates will be reported in the last column, Duplicates .","title":"Output"},{"location":"usage/ls/#example","text":"Please note this sub command may take a while to process depending on the shear number or the size of files existing in a given sub tree. As so, this command should not be run on the head node! Please allocate an interactive node prior to running this command or submit this command as a job via sbatch. # Step 0.) Grab an interactive node # Do not run this on the head node! srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 16gb --cpus-per-task = 4 --pty bash module purge # Step 1.) Find duplicate files ./spacesaver ls /data/CCBR/rawdata/ccbr123/ > ccbr123_ls.tsv","title":"Example"}]}